{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing methods to simulate radial Ornstein-Uhlenbeck bridge \n",
    "This is a specific case of an interest rates model proposed by AÃ¯t-Sahalia and Lo (Journal of Finance, 1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DiffusionBridge as db\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import iv, ivp\n",
    "from torch.distributions.gamma import Gamma\n",
    "from DiffusionBridge.auxiliary import AuxiliaryDiffusion\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify problem settings\n",
    "interval = 1\n",
    "M = 50\n",
    "path_dir = \".\"\n",
    "folder = \"results_radial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem settings\n",
    "d = 1\n",
    "T = torch.tensor(float(interval))\n",
    "theta = torch.tensor(4.0)\n",
    "f = lambda t,x: theta / x - x \n",
    "sigma = torch.tensor(1.0)\n",
    "diffusion = db.diffusion.model(f, sigma, d, T, M)\n",
    "\n",
    "# transition density\n",
    "log_transition_density = lambda t,x,x0: theta * torch.log(x / x0) + 0.5 * torch.log(x * x0) - x**2 \\\n",
    "                                    + (theta + 0.5) * t - torch.log(torch.sinh(t)) \\\n",
    "                                    - (x**2 + x0**2) / (torch.exp(2.0 * t) - 1.0) \\\n",
    "                                    + torch.log(iv(theta - 0.5, x * x0 / torch.sinh(t)))\n",
    "\n",
    "score_transition = lambda t,x,x0: theta / x + 1.0 / (2.0 * x) - 2.0 * x - 2.0 * x / (torch.exp(2.0 * t) - 1.0) \\\n",
    "                                 + (1.0 / iv(theta - 0.5, x * x0 / torch.sinh(t))) * ivp(theta - 0.5, x * x0 / torch.sinh(t)) * x0 / torch.sinh(t)\n",
    "\n",
    "# h-transform\n",
    "grad_logh = lambda t,x,xT: -theta / x + 1.0 / (2.0 * x) - 2.0 * x / (torch.exp(2.0 * (T-t)) - 1.0) \\\n",
    "                        + (1.0 / iv(theta - 0.5, xT * x / torch.sinh(T-t))) * ivp(theta - 0.5, xT * x / torch.sinh(T-t)) * xT / torch.sinh(T-t)\n",
    "\n",
    "# score marginal \n",
    "score_marginal = lambda t,x,x0,xT: score_transition(t, x, x0) + grad_logh(t, x, xT)\n",
    "\n",
    "# sample size\n",
    "N = 2**10\n",
    "\n",
    "# repetitions\n",
    "R = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial and terminal conditions\n",
    "initial_condition = [1.5, 1.5, 3.0]\n",
    "terminal_condition = [1.0, 2.5, 4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn backward diffusion bridge process with score matching\n",
    "distribution_X0 = Gamma(torch.tensor(5.0), torch.tensor(2.0))\n",
    "simulate_X0 = lambda n: distribution_X0.sample((n, )).reshape(n, d)\n",
    "XT = []\n",
    "epsilon = 1.0\n",
    "minibatch = 100\n",
    "num_initial_per_batch = 10\n",
    "num_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "ema_momentum = 0.99\n",
    "\n",
    "output = diffusion.learn_full_score_transition(\n",
    "    simulate_X0, XT, epsilon, minibatch, num_initial_per_batch, num_iterations, learning_rate, ema_momentum\n",
    ")\n",
    "score_transition_net = output[\"net\"]\n",
    "loss_values_transition = output[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate backward diffusion bridge (BDB) process with approximate score\n",
    "BDB = {\n",
    "    measure: torch.zeros(3, R) for measure in [\"ess\", \"logestimate\", \"acceptrate\"]\n",
    "}\n",
    "\n",
    "for c in range(3):\n",
    "    X0 = initial_condition[c] * torch.ones(d)\n",
    "    XT = terminal_condition[c] * torch.ones(d)\n",
    "    for r in range(R):\n",
    "        with torch.no_grad():\n",
    "            output = diffusion.simulate_bridge_backwards(\n",
    "                score_transition_net, X0, XT, epsilon, N, full_score=True,\n",
    "            )\n",
    "            trajectories = output[\"trajectories\"]\n",
    "            log_proposal = output[\"logdensity\"]\n",
    "        log_target = diffusion.law_bridge(trajectories) \n",
    "        log_weights = log_target - log_proposal\n",
    "\n",
    "        # importance sampling\n",
    "        max_log_weights = torch.max(log_weights)\n",
    "        weights = torch.exp(log_weights - max_log_weights)\n",
    "        norm_weights = weights / torch.sum(weights)\n",
    "        ess = 1.0 / torch.sum(norm_weights**2)\n",
    "        log_transition_estimate = torch.log(torch.mean(weights)) + max_log_weights\n",
    "        BDB[\"ess\"][c, r] = ess\n",
    "        BDB[\"logestimate\"][c, r] = log_transition_estimate\n",
    "\n",
    "        # independent Metropolis-Hastings\n",
    "        initial = diffusion.simulate_bridge_backwards(\n",
    "            score_transition_net, X0, XT, epsilon, 1, full_score=True,\n",
    "        )\n",
    "        current_trajectory = initial[\"trajectories\"]\n",
    "        current_log_proposal = initial[\"logdensity\"] \n",
    "        current_log_target = diffusion.law_bridge(current_trajectory)\n",
    "        current_log_weight = current_log_target - current_log_proposal\n",
    "        num_accept = 0\n",
    "        for n in range(N):\n",
    "            proposed_trajectory = trajectories[n, :, :]\n",
    "            proposed_log_weight = log_weights[n]\n",
    "            log_accept_prob = proposed_log_weight - current_log_weight\n",
    "\n",
    "            if (torch.log(torch.rand(1)) < log_accept_prob):\n",
    "                current_trajectory = proposed_trajectory.clone()\n",
    "                current_log_weight = proposed_log_weight.clone()  \n",
    "                num_accept += 1\n",
    "        accept_rate = num_accept / N\n",
    "        BDB[\"acceptrate\"][c, r] = accept_rate\n",
    "\n",
    "        # print\n",
    "        print(\n",
    "            f\"Initial: {initial_condition[c]}\",\n",
    "            f\"Terminal: {terminal_condition[c]}\",\n",
    "            f\"BDB repetition: {r}\",\n",
    "            f\"ESS%: {float(ess * 100 / N):.2f}\",\n",
    "            f\"log-transition: {float(log_transition_estimate):.2f}\",\n",
    "            f\"Accept rate: {float(accept_rate):.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drift functions of existing methods\n",
    "drifts = {}\n",
    "modify = {}\n",
    "\n",
    "# forward diffusion method of Pedersen (1995)\n",
    "drifts[\"FD\"] = f\n",
    "\n",
    "# modified diffusion bridge (MDB) method of Durham and Gallant (2002)\n",
    "drifts[\"MDB\"] = lambda t, x: (XT - x) / (T - t)\n",
    "modify[\"MDB\"] = \"variance\"\n",
    "\n",
    "# diffusion bridge proposal of Clark (1990) and Delyon and Hu (2006)\n",
    "auxiliary_type = \"bm\"\n",
    "initial_params = {\"alpha\": torch.zeros(d)}\n",
    "bm_auxiliary = AuxiliaryDiffusion(\n",
    "    diffusion, auxiliary_type, initial_params, requires_grad=False\n",
    ")\n",
    "drifts[\"CDH\"] = lambda t, x: f(t, x) + diffusion.Sigma * bm_auxiliary.grad_logh(\n",
    "    XT, t, x\n",
    ")\n",
    "modify[\"CDH\"] = \"time\"\n",
    "\n",
    "# learn guided proposal of Schauer, Van Der Meulen and Van Zanten\n",
    "auxiliary_type = \"ou\"\n",
    "initial_params = {\n",
    "    \"alpha\": 4.0 * torch.ones(d),\n",
    "    \"beta\": 2.0 * torch.ones(d),\n",
    "}\n",
    "for c in range(3):\n",
    "    X0 = initial_condition[c] * torch.ones(d)\n",
    "    XT = terminal_condition[c] * torch.ones(d)\n",
    "\n",
    "    # guided proposal with linearization\n",
    "    ou_auxiliary = AuxiliaryDiffusion(diffusion, auxiliary_type, initial_params, requires_grad=False)\n",
    "    drifts[\"GDB\"] = lambda t, x: f(t, x) + diffusion.Sigma * ou_auxiliary.grad_logh(XT, t, x)\n",
    "\n",
    "modify[\"GDB\"] = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate existing methods\n",
    "results = {\"BDB\": BDB}\n",
    "\n",
    "for method, drift in drifts.items():\n",
    "    # measures to store\n",
    "    result = {\n",
    "        measure: torch.zeros(3, R) for measure in [\"ess\", \"logestimate\", \"acceptrate\"]\n",
    "    }\n",
    "    for c in range(3):\n",
    "        X0 = initial_condition[c] * torch.ones(d)\n",
    "        XT = terminal_condition[c] * torch.ones(d)\n",
    "\n",
    "        # repetition\n",
    "        for r in range(R):\n",
    "            with torch.no_grad():\n",
    "                output = diffusion.simulate_proposal_bridge(\n",
    "                    drift, X0, XT, N, modify.get(method)\n",
    "                )\n",
    "            trajectories = output[\"trajectories\"]\n",
    "            if method == \"CDH\":\n",
    "                log_weights = bm_auxiliary.log_radon_nikodym(trajectories)\n",
    "            elif method == \"GDB\":\n",
    "                log_weights = ou_auxiliary.log_radon_nikodym(trajectories)\n",
    "            else:\n",
    "                log_proposal = output[\"logdensity\"]\n",
    "                log_target = diffusion.law_bridge(trajectories)\n",
    "                log_weights = log_target - log_proposal\n",
    "\n",
    "            # importance sampling\n",
    "            max_log_weights = torch.max(log_weights)\n",
    "            weights = torch.exp(log_weights - max_log_weights)\n",
    "            norm_weights = weights / torch.sum(weights)\n",
    "            ess = 1.0 / torch.sum(norm_weights**2)\n",
    "            log_transition_estimate = torch.log(torch.mean(weights)) + max_log_weights\n",
    "            result[\"ess\"][c, r] = ess\n",
    "            result[\"logestimate\"][c, r] = log_transition_estimate\n",
    "\n",
    "            # independent Metropolis-Hastings\n",
    "            initial = diffusion.simulate_proposal_bridge(\n",
    "                drift, X0, XT, 1, modify.get(method)\n",
    "            )\n",
    "            current_trajectory = initial[\"trajectories\"]\n",
    "            if method == \"CDH\":\n",
    "                current_log_weight = bm_auxiliary.log_radon_nikodym(current_trajectory)\n",
    "            elif method == \"GDB\":\n",
    "                current_log_weight = ou_auxiliary.log_radon_nikodym(current_trajectory)\n",
    "            else:\n",
    "                current_log_proposal = initial[\"logdensity\"]\n",
    "                current_log_target = diffusion.law_bridge(current_trajectory)\n",
    "                current_log_weight = current_log_target - current_log_proposal\n",
    "            num_accept = 0\n",
    "            for n in range(N):\n",
    "                proposed_trajectory = trajectories[n, :, :]\n",
    "                proposed_log_weight = log_weights[n]\n",
    "                log_accept_prob = proposed_log_weight - current_log_weight\n",
    "                if torch.log(torch.rand(1)) < log_accept_prob:\n",
    "                    current_trajectory = proposed_trajectory.clone()\n",
    "                    current_log_weight = proposed_log_weight.clone()\n",
    "                    num_accept += 1\n",
    "            accept_rate = num_accept / N\n",
    "            result[\"acceptrate\"][c, r] = accept_rate\n",
    "\n",
    "            # print\n",
    "            print(\n",
    "                f\"Initial: {initial_condition[c]}\",\n",
    "                f\"Terminal: {terminal_condition[c]}\",\n",
    "                f\"{method} repetition: {r}\",\n",
    "                f\"ESS%: {float(ess * 100 / N):.2f}\",\n",
    "                f\"log-transition: {float(log_transition_estimate):.2f}\",\n",
    "                f\"Accept rate: {float(accept_rate):.4f}\",\n",
    "            )\n",
    "\n",
    "    # store result\n",
    "    results[method] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(3):\n",
    "    X0 = initial_condition[c] * torch.ones(d)\n",
    "    XT = terminal_condition[c] * torch.ones(d)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\n",
    "        f\"Initial: {initial_condition[c]}\",\n",
    "        f\"Terminal: {terminal_condition[c]}\",\n",
    "    )\n",
    "    print(\"-\" * 30)\n",
    "    for method, result in results.items():\n",
    "        # compare ESS\n",
    "        print(\n",
    "            f\"{method}\",\n",
    "            f\"ESS%: {float(torch.mean(result['ess'][c, :]) * 100 / N):.2f}\",\n",
    "        )\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # compare RMSE of log-transition density\n",
    "    for method, result in results.items():\n",
    "        RMSE = float(\n",
    "            torch.sqrt(torch.mean((result['logestimate'][c, :] - log_transition_density(T, XT, X0)) ** 2))\n",
    "        )\n",
    "        print(\n",
    "            f\"{method}\",\n",
    "            f\"{RMSE:.4f}\",\n",
    "        )\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # compare indepedent Meteropolis-Hastings acceptance rate\n",
    "    for method, result in results.items():\n",
    "        print(\n",
    "            f\"{method}\",\n",
    "            f\"Accept rate%: {float(torch.mean(result['acceptrate'][c, :]) * 100):.2f}\",\n",
    "        )\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "file_name = f\"{path_dir}/{folder}/radial_T{interval}.pt\"\n",
    "torch.save(results, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
