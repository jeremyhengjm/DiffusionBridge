{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing methods to simulate Ornstein-Uhlenbeck bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DiffusionBridge as db\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from DiffusionBridge.utils import normal_logpdf\n",
    "from DiffusionBridge.auxiliary import AuxiliaryDiffusion\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify problem settings\n",
    "d = 1\n",
    "interval = 1\n",
    "M = 50\n",
    "num_iterations = 500\n",
    "pos_dim = 16\n",
    "terminal_std = None\n",
    "path_dir = \".\"\n",
    "folder = \"results_ou\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time interval\n",
    "T = torch.tensor(float(interval))\n",
    "\n",
    "# diffusion model\n",
    "alpha = torch.tensor(0.0)\n",
    "beta = torch.tensor(2.0)\n",
    "f = lambda t,x: alpha - beta * x \n",
    "sigma = torch.tensor(1.0)\n",
    "diffusion = db.diffusion.model(f, sigma, d, T, M)\n",
    "\n",
    "# initial and terminal constraints\n",
    "X0 = 1.0 * torch.ones(d)\n",
    "XT = 1.0 * torch.ones(d)\n",
    "\n",
    "# transition density\n",
    "ratio = alpha / beta\n",
    "transition_mean = lambda t, x: ratio + (x - ratio) * torch.exp(-beta * t)\n",
    "transition_var = lambda t: (1.0 - torch.exp(-2.0 * beta * t)) / (2.0 * beta)\n",
    "score_transition = lambda t, x: (transition_mean(t, X0) - x) / transition_var(t)\n",
    "\n",
    "# terminal constraint\n",
    "if terminal_std:\n",
    "    XT = (\n",
    "        float(terminal_std) * torch.sqrt(transition_var(T)) + transition_mean(T, X0)\n",
    "    ) * torch.ones(d)\n",
    "print(f\"terminal state: {float(XT):.4f}\")\n",
    "\n",
    "# transition density from X0 to XT\n",
    "log_transition_density = normal_logpdf(\n",
    "    XT.reshape(1, d), transition_mean(T, X0), transition_var(T)\n",
    ")\n",
    "print(f\"log-transition: {float(log_transition_density):.4f}\")\n",
    "\n",
    "# marginal density\n",
    "marginal_var = lambda t: 1.0 / (1.0 / transition_var(t) + torch.exp(- 2.0 * beta * (T-t)) / transition_var(T-t))\n",
    "marginal_mean = lambda t: (transition_mean(t,X0) / transition_var(t) + XT * torch.exp(- beta * (T-t)) / transition_var(T-t)) * marginal_var(t) \n",
    "score_marginal = lambda t,x: (marginal_mean(t) - x) / marginal_var(t)\n",
    "grad_logh = lambda t,x: (XT - transition_mean(T - t, x)) * torch.exp(- beta * (T - t)) / transition_var(T - t)\n",
    "\n",
    "# sample size\n",
    "N = 2**10\n",
    "\n",
    "# repetitions\n",
    "R = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn backward diffusion bridge process with score matching\n",
    "epsilon = 1.0\n",
    "minibatch = 100\n",
    "learning_rate = 0.01\n",
    "ema_momentum = 0.99\n",
    "network_config = {\"pos_dim\": pos_dim}\n",
    "output = diffusion.learn_score_transition(\n",
    "    X0,\n",
    "    XT,\n",
    "    epsilon,\n",
    "    minibatch,\n",
    "    num_iterations,\n",
    "    learning_rate,\n",
    "    ema_momentum,\n",
    "    network_config,\n",
    ")\n",
    "score_transition_net = output[\"net\"]\n",
    "\n",
    "# simulate backward diffusion bridge (BDB) process with approximate score\n",
    "BDB = {\n",
    "    measure: torch.zeros(R) for measure in [\"ess\", \"logestimate\", \"acceptrate\"]\n",
    "}\n",
    "for r in range(R):\n",
    "    with torch.no_grad():\n",
    "        output = diffusion.simulate_bridge_backwards(\n",
    "            score_transition_net, X0, XT, epsilon, N\n",
    "        )\n",
    "        trajectories = output[\"trajectories\"]\n",
    "        log_proposal = output[\"logdensity\"]\n",
    "    log_target = diffusion.law_bridge(trajectories) \n",
    "    log_weights = log_target - log_proposal\n",
    "\n",
    "    # importance sampling\n",
    "    max_log_weights = torch.max(log_weights)\n",
    "    weights = torch.exp(log_weights - max_log_weights)\n",
    "    norm_weights = weights / torch.sum(weights)\n",
    "    ess = 1.0 / torch.sum(norm_weights**2)\n",
    "    log_transition_estimate = torch.log(torch.mean(weights)) + max_log_weights\n",
    "    BDB[\"ess\"][r] = ess\n",
    "    BDB[\"logestimate\"][r] = log_transition_estimate\n",
    "\n",
    "    # independent Metropolis-Hastings\n",
    "    initial = diffusion.simulate_bridge_backwards(\n",
    "        score_transition_net, X0, XT, epsilon, 1\n",
    "    )\n",
    "    current_trajectory = initial[\"trajectories\"]\n",
    "    current_log_proposal = initial[\"logdensity\"] \n",
    "    current_log_target = diffusion.law_bridge(current_trajectory)\n",
    "    current_log_weight = current_log_target - current_log_proposal\n",
    "    num_accept = 0\n",
    "    for n in range(N):\n",
    "        proposed_trajectory = trajectories[n, :, :]\n",
    "        proposed_log_weight = log_weights[n]\n",
    "        log_accept_prob = proposed_log_weight - current_log_weight\n",
    "\n",
    "        if (torch.log(torch.rand(1)) < log_accept_prob):\n",
    "            current_trajectory = proposed_trajectory.clone()\n",
    "            current_log_weight = proposed_log_weight.clone()  \n",
    "            num_accept += 1\n",
    "    accept_rate = num_accept / N\n",
    "    BDB[\"acceptrate\"][r] = accept_rate\n",
    "\n",
    "    # print\n",
    "    print(\n",
    "        f\"BDB repetition: {r}\",\n",
    "        f\"ESS%: {float(ess * 100 / N):.2f}\",\n",
    "        f\"log-transition: {float(log_transition_estimate):.2f}\",\n",
    "        f\"Accept rate: {float(accept_rate):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn forward diffusion bridge process with score matching\n",
    "epsilon = 1.0\n",
    "minibatch = 100\n",
    "learning_rate = 0.01\n",
    "ema_momentum = 0.99\n",
    "network_config = {\"pos_dim\": pos_dim}\n",
    "output = diffusion.learn_score_marginal(\n",
    "    score_transition_net,\n",
    "    X0,\n",
    "    XT,\n",
    "    epsilon,\n",
    "    minibatch,\n",
    "    num_iterations,\n",
    "    learning_rate,\n",
    "    ema_momentum,\n",
    "    network_config,\n",
    ")\n",
    "score_marginal_net = output[\"net\"]\n",
    "\n",
    "# simulate forward diffusion bridge (FDB) process using approximate score\n",
    "FDB = {\n",
    "    measure: torch.zeros(R) for measure in [\"ess\", \"logestimate\", \"acceptrate\"]\n",
    "}\n",
    "for r in range(R):\n",
    "    with torch.no_grad():\n",
    "        output = diffusion.simulate_bridge_forwards(\n",
    "            score_transition_net, score_marginal_net, X0, XT, epsilon, N\n",
    "        )\n",
    "        trajectories = output[\"trajectories\"]\n",
    "        log_proposal = output[\"logdensity\"]\n",
    "    log_target = diffusion.law_bridge(trajectories) \n",
    "    log_weights = log_target - log_proposal\n",
    "\n",
    "    # importance sampling\n",
    "    max_log_weights = torch.max(log_weights)\n",
    "    weights = torch.exp(log_weights - max_log_weights)\n",
    "    norm_weights = weights / torch.sum(weights)\n",
    "    ess = 1.0 / torch.sum(norm_weights**2)\n",
    "    log_transition_estimate = torch.log(torch.mean(weights)) + max_log_weights\n",
    "    FDB[\"ess\"][r] = ess\n",
    "    FDB[\"logestimate\"][r] = log_transition_estimate\n",
    "\n",
    "    # independent Metropolis-Hastings\n",
    "    initial = diffusion.simulate_bridge_forwards(\n",
    "        score_transition_net, score_marginal_net, X0, XT, epsilon, 1\n",
    "    )\n",
    "    current_trajectory = initial[\"trajectories\"]\n",
    "    current_log_proposal = initial[\"logdensity\"] \n",
    "    current_log_target = diffusion.law_bridge(current_trajectory)\n",
    "    current_log_weight = current_log_target - current_log_proposal\n",
    "    num_accept = 0\n",
    "    for n in range(N):\n",
    "        proposed_trajectory = trajectories[n, :, :]\n",
    "        proposed_log_weight = log_weights[n]\n",
    "        log_accept_prob = proposed_log_weight - current_log_weight\n",
    "\n",
    "        if (torch.log(torch.rand(1)) < log_accept_prob):\n",
    "            current_trajectory = proposed_trajectory.clone()\n",
    "            current_log_weight = proposed_log_weight.clone()  \n",
    "            num_accept += 1\n",
    "    accept_rate = num_accept / N\n",
    "    FDB[\"acceptrate\"][r] = accept_rate\n",
    "\n",
    "    # print\n",
    "    print(\n",
    "        f\"FDB repetition: {r}\",\n",
    "        f\"ESS%: {float(ess * 100 / N):.2f}\",\n",
    "        f\"log-transition: {float(log_transition_estimate):.2f}\",\n",
    "        f\"Accept rate: {float(accept_rate):.4f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drift functions of existing methods\n",
    "drifts = {}\n",
    "modify = {}\n",
    "\n",
    "# forward diffusion method of Pedersen (1995)\n",
    "drifts[\"FD\"] = f\n",
    "\n",
    "# modified diffusion bridge (MDB) method of Durham and Gallant (2002)\n",
    "drifts[\"MDB\"] = lambda t, x: (XT - x) / (T - t)\n",
    "modify[\"MDB\"] = \"variance\"\n",
    "\n",
    "# diffusion bridge proposal of Clark (1990) and Delyon and Hu (2006)\n",
    "auxiliary_type = \"bm\"\n",
    "initial_params = {\n",
    "    \"alpha\": torch.zeros(d)\n",
    "}\n",
    "bm_auxiliary = AuxiliaryDiffusion(\n",
    "    diffusion, auxiliary_type, initial_params, requires_grad=False\n",
    ")\n",
    "drifts[\"CDH\"] = lambda t, x: f(t, x) + diffusion.Sigma * bm_auxiliary.grad_logh(XT, t, x)\n",
    "modify[\"CDH\"] = \"time\"\n",
    "\n",
    "# learn guided proposal of Schauer, Van Der Meulen and Van Zanten\n",
    "auxiliary_type = \"ou\"\n",
    "initial_params = {\n",
    "    \"alpha\": alpha * torch.ones(d),\n",
    "    \"beta\": beta * torch.ones(d),\n",
    "}\n",
    "ou_auxiliary = AuxiliaryDiffusion(\n",
    "    diffusion, auxiliary_type, initial_params, requires_grad=False\n",
    ")\n",
    "\n",
    "drifts[\"GDB\"] = lambda t, x: f(t, x) + diffusion.Sigma * ou_auxiliary.grad_logh(XT, t, x)\n",
    "modify[\"GDB\"] = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate existing methods\n",
    "results = {\"BDB\": BDB, \"FDB\": FDB}\n",
    "\n",
    "for method, drift in drifts.items():\n",
    "    # measures to store\n",
    "    result = {\n",
    "        measure: torch.zeros(R) for measure in [\"ess\", \"logestimate\", \"acceptrate\"]\n",
    "    }\n",
    "\n",
    "    # repetition\n",
    "    for r in range(R):\n",
    "        with torch.no_grad():\n",
    "            output = diffusion.simulate_proposal_bridge(drift, X0, XT, N, modify.get(method))\n",
    "        trajectories = output[\"trajectories\"]\n",
    "        if method == \"CDH\":\n",
    "            log_weights = bm_auxiliary.log_radon_nikodym(trajectories)\n",
    "        elif method == \"GDB\":\n",
    "            log_weights = ou_auxiliary.log_radon_nikodym(trajectories)\n",
    "        else:\n",
    "            log_proposal = output[\"logdensity\"]\n",
    "            log_target = diffusion.law_bridge(trajectories)\n",
    "            log_weights = log_target - log_proposal\n",
    "\n",
    "        # importance sampling\n",
    "        max_log_weights = torch.max(log_weights)\n",
    "        weights = torch.exp(log_weights - max_log_weights)\n",
    "        norm_weights = weights / torch.sum(weights)\n",
    "        ess = 1.0 / torch.sum(norm_weights**2)\n",
    "        log_transition_estimate = torch.log(torch.mean(weights)) + max_log_weights\n",
    "        result[\"ess\"][r] = ess\n",
    "        result[\"logestimate\"][r] = log_transition_estimate\n",
    "\n",
    "        # independent Metropolis-Hastings\n",
    "        initial = diffusion.simulate_proposal_bridge(\n",
    "            drift, X0, XT, 1, modify.get(method)\n",
    "        )\n",
    "        current_trajectory = initial[\"trajectories\"]\n",
    "        if method == \"CDH\":\n",
    "            current_log_weight = bm_auxiliary.log_radon_nikodym(current_trajectory)\n",
    "        elif method == \"GDB\":\n",
    "            current_log_weight = ou_auxiliary.log_radon_nikodym(current_trajectory)\n",
    "        else:\n",
    "            current_log_proposal = initial[\"logdensity\"]\n",
    "            current_log_target = diffusion.law_bridge(current_trajectory)\n",
    "            current_log_weight = current_log_target - current_log_proposal\n",
    "        num_accept = 0\n",
    "        for n in range(N):\n",
    "            proposed_trajectory = trajectories[n, :, :]\n",
    "            proposed_log_weight = log_weights[n]\n",
    "            log_accept_prob = proposed_log_weight - current_log_weight\n",
    "            if torch.log(torch.rand(1)) < log_accept_prob:\n",
    "                current_trajectory = proposed_trajectory.clone()\n",
    "                current_log_weight = proposed_log_weight.clone()\n",
    "                num_accept += 1\n",
    "        accept_rate = num_accept / N\n",
    "        result[\"acceptrate\"][r] = accept_rate\n",
    "\n",
    "        # print\n",
    "        print(\n",
    "            f\"{method} repetition: {r}\",\n",
    "            f\"ESS%: {float(ess * 100 / N):.2f}\",\n",
    "            f\"log-transition: {float(log_transition_estimate):.2f}\",\n",
    "            f\"Accept rate: {float(accept_rate):.4f}\",\n",
    "        )\n",
    "\n",
    "    # store result\n",
    "    results[method] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ESS\n",
    "for method, result in results.items():\n",
    "    print(\n",
    "        f\"{method}\", f\"ESS%: {float(torch.mean(result['ess']) * 100 / N):.2f}\",\n",
    "    )\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# compare RMSE of log-transition density\n",
    "for method, result in results.items():\n",
    "    RMSE = float(\n",
    "        torch.sqrt(torch.mean((result['logestimate'] - log_transition_density) ** 2))\n",
    "    )\n",
    "    print(\n",
    "        f\"{method}\", f\"{RMSE:.4f}\",\n",
    "    )\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# compare independent Meteropolis-Hastings acceptance rate\n",
    "for method, result in results.items():\n",
    "    print(\n",
    "        f\"{method}\", f\"Accept rate%: {float(torch.mean(result['acceptrate']) * 100):.2f}\",\n",
    "    )\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "file_name = f\"{path_dir}/{folder}/ou_dim{d}_T{interval}\"\n",
    "if terminal_std:\n",
    "    file_name += f\"_std{terminal_std}\"\n",
    "torch.save(results, file_name + \".pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
